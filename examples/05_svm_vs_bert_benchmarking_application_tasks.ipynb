{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch version 1.6.0 available.\n"
     ]
    }
   ],
   "source": [
    "from toiro import classifiers\n",
    "from toiro import datadownloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yahoo_movie_reviews (yahoo-movie-reviews.json.tar.gz) exists in /home/taishi-i/toiro_resources .\n",
      "   0                                                  1\n",
      "0  0  笠原郁(榮倉奈々)は、高校時代に助けてくれた図書隊員の“王子様”に憧れて入隊する。顔を憶えて...\n",
      "1  0  原作を読んでいないので、映画を見た批評です。出会い系サイトでの出会い現代社会ではそういったネ...\n",
      "2  0                             CGは相当すごいが筋や演出は期待はずれかな。\n",
      "3  0  「ギャグを散りばめるのはいいけど、一本筋くらい通せよ。真面目なとこは真面目に、笑わせるとこは...\n",
      "4  0  一体、この映画に感動し涙を流した観客は、この映画の何処に感動したのだろうか？史実としての大東...\n"
     ]
    }
   ],
   "source": [
    "# Download the yahoo movie review corpus and load it as pandas\n",
    "corpus = 'yahoo_movie_reviews'\n",
    "datadownloader.download_corpus(corpus)\n",
    "train_df, dev_df, test_df = datadownloader.load_corpus(corpus)\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'toiro.classifiers' has no attribute 'SVMClassificationModel'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-0566501394d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Initialize a SVM model with the Janome tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifiers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSVMClassificationModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mecab-python3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Training a SVM model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'toiro.classifiers' has no attribute 'SVMClassificationModel'"
     ]
    }
   ],
   "source": [
    "# Initialize a SVM model with the Janome tokenizer                              \n",
    "model = classifiers.SVMClassificationModel(tokenizer='mecab-python3')\n",
    "                                                                                \n",
    "# Training a SVM model                                                          \n",
    "model.fit(train_df, dev_df)                                                     \n",
    "                                                                                \n",
    "# Evaluate the trained SVM model                                                \n",
    "svm_result = model.eval(test_df)                                                    \n",
    "print(svm_result)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "text = \"面白い映画でした\"\n",
    "pred_y = model.predict(text) \n",
    "print(pred_y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/cl-tohoku/bert-base-japanese-whole-word-masking/config.json from cache at /home/taishi-i/.cache/torch/transformers/c96f5e731b9f4dc2e8263336947ec74b6f93917c0b9db6e9cf974a8a945dd313.48bc8d0b377948cc990335b8cccbcce084039c1b792bea3d0da671abfc6a3fe5\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading file https://s3.amazonaws.com/models.huggingface.co/bert/cl-tohoku/bert-base-japanese-whole-word-masking/vocab.txt from cache at /home/taishi-i/.cache/torch/transformers/72ee6ecba54b20bba483760db4f23b836f27a6afda54ede38c488e8514bb3705.5fac9da4d8565963664ed9744688dc7008ff5ec4045f604e9515896f9fe46d9c\n",
      "loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/cl-tohoku/bert-base-japanese-whole-word-masking/config.json from cache at /home/taishi-i/.cache/torch/transformers/c96f5e731b9f4dc2e8263336947ec74b6f93917c0b9db6e9cf974a8a945dd313.48bc8d0b377948cc990335b8cccbcce084039c1b792bea3d0da671abfc6a3fe5\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file https://cdn.huggingface.co/cl-tohoku/bert-base-japanese-whole-word-masking/pytorch_model.bin from cache at /home/taishi-i/.cache/torch/transformers/4801b91bfd2de0a4478e02e4a2404e982bd3d6773bc2935eea468f9c644fb593.fbee186ac33c194356f35147fe415d5392623f762cc279e97cecc31b90238cb3\n",
      "All model checkpoint weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/2 * Epoch (train): 100% 1910/1910 [14:32<00:00,  2.19it/s, accuracy01=1.000, loss=0.013]\n",
      "1/2 * Epoch (valid): 100% 239/239 [00:39<00:00,  6.03it/s, accuracy01=0.900, loss=0.306]\n",
      "[2020-08-13 11:46:03,994] \n",
      "1/2 * Epoch 1 (_base): lr=5.000e-05 | momentum=0.9000\n",
      "1/2 * Epoch 1 (train): accuracy01=0.8892 | loss=0.2567\n",
      "1/2 * Epoch 1 (valid): accuracy01=0.9144 | loss=0.2115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1/2 * Epoch 1 (_base): lr=5.000e-05 | momentum=0.9000\n",
      "1/2 * Epoch 1 (train): accuracy01=0.8892 | loss=0.2567\n",
      "1/2 * Epoch 1 (valid): accuracy01=0.9144 | loss=0.2115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 * Epoch (train): 100% 1910/1910 [15:09<00:00,  2.10it/s, accuracy01=1.000, loss=0.032]\n",
      "2/2 * Epoch (valid): 100% 239/239 [00:40<00:00,  5.92it/s, accuracy01=0.900, loss=0.325]\n",
      "[2020-08-13 12:02:10,475] \n",
      "2/2 * Epoch 2 (_base): lr=5.000e-05 | momentum=0.9000\n",
      "2/2 * Epoch 2 (train): accuracy01=0.9543 | loss=0.1246\n",
      "2/2 * Epoch 2 (valid): accuracy01=0.9251 | loss=0.2208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2/2 * Epoch 2 (_base): lr=5.000e-05 | momentum=0.9000\n",
      "2/2 * Epoch 2 (train): accuracy01=0.9543 | loss=0.1246\n",
      "2/2 * Epoch 2 (valid): accuracy01=0.9251 | loss=0.2208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top best models:\n",
      "log_dir/checkpoints/train.1.pth\t0.2115\n",
      "accuracy_score: 0.9374017810371923\n",
      "macro_f1_score: 0.9373945615237904\n",
      "classification_report:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.96      0.94      1890\n",
      "           1       0.96      0.92      0.94      1928\n",
      "\n",
      "    accuracy                           0.94      3818\n",
      "   macro avg       0.94      0.94      0.94      3818\n",
      "weighted avg       0.94      0.94      0.94      3818\n",
      "\n",
      "elapsed_time: 1891.0249490737915\n"
     ]
    }
   ],
   "source": [
    "# Initialize a BERT model \n",
    "model = classifiers.BERTClassificationModel()\n",
    "\n",
    "# Training a BERT model\n",
    "model.fit(train_df, dev_df, log_dir=\"./log_dir\", max_seq_length=256, epochs=2, verbose=True)\n",
    "\n",
    "# Evaluate the trained BERT model \n",
    "bert_result = model.eval(test_df)\n",
    "print(bert_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "text = \"面白い映画でした\"\n",
    "pred_y = model.predict(text) \n",
    "print(pred_y) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
